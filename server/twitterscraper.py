# -*- coding: utf-8 -*-
"""twitterScraper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXPbt770CnFQoahkvstBMqmo2nlKk5Ut
"""

!pip install twitterscraper

!pip install vaderSentiment

!pip install gensim

from gensim.summarization import keywords
import sys
import os
import re

from tqdm import tqdm
import numpy as np
import pandas as pd

from twitterscraper import query_tweets
import datetime as dt

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
  score = analyzer.polarity_scores(sentence)
  return score

def typecast(keyword,time_start=dt.date(2020,1,1),time_end=dt.date(2020,1,11),negative,number_of_tweets):
  # Scraping Tweets
  tweets = query_tweets(keyword,begindate=time_start,enddate=time_end, limit=number_of_tweets, lang='english')

  # Turning tweets into a Pandas DataFrame
  df = pd.DataFrame(t.__dict__ for t in tweets)  

  # Break dataframe into text and url's
  tweet_content = df['text']
  tweet_url = df['tweet_url']

  # Sentiment Analysis
  array_of_scores_for_tweets = []
  for tweet in tweet_content:
    midstep = sentiment_analyzer_scores(tweet)
    score = midstep['compound']
    array_of_scores_for_tweets.append(score)

  final_list_for_scores = []
  final_list_for_tweets = []
  if (negative):
    for i in range(len(array_of_scores_for_tweets)-1):
      if (array_of_scores_for_tweets[i] < -0.2):
        final_list_for_scores.append(array_of_scores_for_tweets[i])
        final_list_for_tweets.append(tweet_content[i])
  else:
    final_list_for_scores = array_of_scores_for_tweets
    final_list_for_tweets = tweet_content
    
   # Negative Word Perception
    negativeTweetCount = 0
    totalTweetCount = 0
    for i in range(len(array_of_scores_for_tweets)-1):
        if (array_of_scores_for_tweets[i] < -0.15):
            negativeWordCount += 1
        totalTweetCount += 1
        
  #Keyword Analysis
  keywords_of_tweets = []
  keyword_of_indivdual_tweet = []
  for tweet in final_list_for_tweets:
    keywords_of_tweet = keywords(tweet).split()
    keyword_of_indivdual_tweet.append(keywords_of_tweet)
    for word in keywords_of_tweet:
      keywords_of_tweets.append(word)

  #TextRank
  unique_list = dict.fromkeys(keywords_of_tweets)
  for word in unique_list:
    unique_list[word] = 0

  for word in keywords_of_tweets:
    unique_list[word] += 1
  
  return [tweet_url, tweet_content, keyword_of_indivdual_tweet, unique_list, negativeWordCount, totalTweetCount]

begin_date = dt.date(2020,1,1)
end_date = dt.date(2020,1,11)
tweet_links, tweet_content, indivdual_tweet_keyword, keyword_analysis = 1,2,3,4
unpack = typecast(keyword="askrbc",time_start=begin_date,time_end=end_date,negative=False,number_of_tweets=1000)

tweet_links, tweet_content, indivdual_tweet_keyword, keyword_analysis = unpack[0], unpack[1], unpack[2], unpack[3], unpack[4], unpack[5]
