# -*- coding: utf-8 -*-
"""twitterScraper

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXPbt770CnFQoahkvstBMqmo2nlKk5Ut
"""

#!pip install twitterscraper

#!pip install vaderSentiment

#!pip install gensim

from gensim.summarization import keywords
import sys
import os
import re

import numpy as np
import pandas as pd

from twitterscraper import query_tweets
import datetime as dt

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
  score = analyzer.polarity_scores(sentence)
  return score

def typecast(keyword,negative,number_of_tweets):
  # Scraping Tweets
  number_of_tweets= int(number_of_tweets)
  tweets = query_tweets(keyword,begindate=dt.datetime(2020,1,1),enddate=dt.datetime(2020,1,11), lang='english', limit=number_of_tweets)
  # Turning tweets into a Pandas DataFrame
  df = pd.DataFrame(t.__dict__ for t in tweets)  

  # Break dataframe into text and url's
  tweet_content = df['text']
  tweet_url = df['tweet_url']

  # Sentiment Analysis
  array_of_scores_for_tweets = []
  for tweet in tweet_content:
    midstep = sentiment_analyzer_scores(tweet)
    score = midstep['compound']
    array_of_scores_for_tweets.append(score)

  final_list_for_scores = []
  final_list_for_tweets = []
  if (negative):
    for i in range(len(array_of_scores_for_tweets)-1):
      if (array_of_scores_for_tweets[i] < -0.2):
        final_list_for_scores.append(array_of_scores_for_tweets[i])
        final_list_for_tweets.append(tweet_content[i])
  else:
    final_list_for_scores = array_of_scores_for_tweets
    final_list_for_tweets = tweet_content

  # Negative Word Perception
  negativeTweetCount = 0
  totalTweetCount = 0
  for i in range(len(array_of_scores_for_tweets)-1):
    if (array_of_scores_for_tweets[i] < -0.15):
      negativeTweetCount+= 1
      totalTweetCount += 1

  #Keyword Analysis
  keywords_of_tweets = []
  keyword_of_indivdual_tweet = []
  for tweet in final_list_for_tweets:
    keywords_of_tweet = keywords(tweet).split()
    keyword_of_indivdual_tweet.append(keywords_of_tweet)
    for word in keywords_of_tweet:
      keywords_of_tweets.append(word)

  #TextRank
  unique_list = dict.fromkeys(keywords_of_tweets)
  for word in unique_list:
    unique_list[word] = 0

  for word in keywords_of_tweets:
    unique_list[word] += 1

  key_content = []
  for i in range(totalTweetCount):
    new_list = {}
    new_list['id'] = tweet_url[i]
    new_list['content'] = tweet_content[i]
    key_content.append(new_list)
  return {"tweets":key_content,"allKeywords":keyword_of_indivdual_tweet,  "uniqueList":unique_list, "noNegativeTweets":negativeTweetCount,"tweetsAnalyzed": totalTweetCount}

##begin_date = dt.date(2020,1,1)
#end_date = dt.date(2020,1,11)
#tweet_links, tweet_content, indivdual_tweet_keyword, keyword_analysis = 1,2,3,4
#unpack = typecast(keyword="askrbc",time_start=begin_date,time_end=end_date,negative=False,number_of_tweets=1000)

#tweet_links, tweet_content, indivdual_tweet_keyword, keyword_analysis = unpack[0], unpack[1], unpack[2], unpack[3]
